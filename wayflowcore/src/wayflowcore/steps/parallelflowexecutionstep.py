# Copyright Â© 2025 Oracle and/or its affiliates.
#
# This software is under the Apache License 2.0
# (LICENSE-APACHE or http://www.apache.org/licenses/LICENSE-2.0) or Universal Permissive License
# (UPL) 1.0 (LICENSE-UPL or https://oss.oracle.com/licenses/upl), at your option.

import logging
import warnings
from typing import TYPE_CHECKING, Any, Dict, List, Optional, Set

from pyagentspec.property import json_schemas_have_same_type

from wayflowcore._metadata import MetadataType
from wayflowcore._utils.async_helpers import run_async_function_in_parallel
from wayflowcore.executors._flowexecutor import FlowConversationExecutor
from wayflowcore.executors.executionstatus import ExecutionStatus, FinishedStatus
from wayflowcore.executors.interrupts.executioninterrupt import InterruptedExecutionStatus
from wayflowcore.property import Property
from wayflowcore.steps.step import Step, StepExecutionStatus, StepResult
from wayflowcore.tools import Tool

if TYPE_CHECKING:
    from wayflowcore.executors._flowconversation import FlowConversation
    from wayflowcore.flow import Flow


logger = logging.getLogger(__name__)
_MAX_NUM_WORKERS = 20


class ParallelFlowExecutionStep(Step):
    """Executes several flows in parallel inside a step."""

    def __init__(
        self,
        flows: List["Flow"],
        max_workers: Optional[int] = None,
        input_descriptors: Optional[List[Property]] = None,
        output_descriptors: Optional[List[Property]] = None,
        input_mapping: Optional[Dict[str, str]] = None,
        output_mapping: Optional[Dict[str, str]] = None,
        name: Optional[str] = None,
        __metadata_info__: Optional[MetadataType] = None,
    ):
        """
        Note
        ----

        A step has input and output descriptors, describing what values the step requires to run and what values it produces.

        **Input descriptors**

        By default, when ``input_descriptors`` is set to ``None``, the input_descriptors is set to the union of the
        input descriptors of the subflows of this step. Inputs of subflows that have the same name and type are merged
        together. However, if the input name matches, but the type is not the same, an error will be thrown.
        See :ref:`Flow <Flow>` to learn more about how flow inputs are resolved.

        If you provide a list of input descriptors, each provided descriptor will automatically override the detected one,
        in particular using the new type instead of the detected one.
        If some of them are missing, an error will be thrown at instantiation of the step.

        If you provide input descriptors for non-autodetected variables, a warning will be emitted, and
        they won't be used during the execution of the step.

        **Output descriptors**

        The outputs descriptors of this step are the union of all the outputs generated by all the inner flows.
        If outputs of different subflows have the same name, an error will be thrown.
        See :ref:`Flow <Flow>` to learn more about how flow outputs are resolved.

        Parameters
        ----------
        flows:
            ``Flow`` s that the step needs to execute in parallel.
        max_workers:
            Number of workers to use if parallel execution is enabled.
            If None, the number of threads set in the `initialize_threadpool` is used.
            If `initialize_threadpool` was not called with an explicit number of threads, 20 is used as upper limit.
        input_descriptors:
            Input descriptors of the step. ``None`` means the step will resolve the input descriptors automatically
            using its static configuration in a best effort manner.

        output_descriptors:
            Output descriptors of the step. ``None`` means the step will resolve them automatically using its static
            configuration in a best effort manner.

        name:
            Name of the step.

        input_mapping:
            Mapping between the name of the inputs this step expects and the name to get it from in the
            conversation input/output dictionary.

        output_mapping:
            Mapping between the name of the outputs this step expects and the name to get it from in the
            conversation input/output dictionary.

        Example
        -------
        In the following example, we will perform the 4 basic math operations on the same couple of inputs in parallel:

        >>> from wayflowcore import Flow
        >>> from wayflowcore.property import IntegerProperty
        >>> from wayflowcore.steps import ParallelFlowExecutionStep
        >>> from wayflowcore.steps import ToolExecutionStep
        >>> from wayflowcore.tools.toolhelpers import DescriptionMode, ServerTool
        >>>
        >>> add = ServerTool(
        ...     name="add",
        ...     description="Sum two numbers",
        ...     input_descriptors=[IntegerProperty(name="a"), IntegerProperty(name="b")],
        ...     output_descriptors=[IntegerProperty(name="sum")],
        ...     func=lambda a, b: a + b,
        ... )
        >>> subtract = ServerTool(
        ...     name="subtract",
        ...     description="Subtract two numbers",
        ...     input_descriptors=[IntegerProperty(name="a"), IntegerProperty(name="b")],
        ...     output_descriptors=[IntegerProperty(name="difference")],
        ...     func=lambda a, b: a - b,
        ... )
        >>> multiply = ServerTool(
        ...     name="multiply",
        ...     description="Multiply two numbers",
        ...     input_descriptors=[IntegerProperty(name="a"), IntegerProperty(name="b")],
        ...     output_descriptors=[IntegerProperty(name="product")],
        ...     func=lambda a, b: a * b,
        ... )
        >>> divide = ServerTool(
        ...     name="divide",
        ...     description="Divide two numbers",
        ...     input_descriptors=[IntegerProperty(name="a"), IntegerProperty(name="b")],
        ...     output_descriptors=[IntegerProperty(name="quotient")],
        ...     func=lambda a, b: a // b,
        ... )
        >>> sum_flow = Flow.from_steps([ToolExecutionStep(name="sum_step", tool=add)])
        >>> subtract_flow = Flow.from_steps([ToolExecutionStep(name="subtract_step", tool=subtract)])
        >>> multiply_flow = Flow.from_steps([ToolExecutionStep(name="multiply_step", tool=multiply)])
        >>> divide_flow = Flow.from_steps([ToolExecutionStep(name="divide_step", tool=divide)])
        >>> parallel_flow_step = ParallelFlowExecutionStep(
        ...    name="parallel_flow_step",
        ...    flows=[sum_flow, subtract_flow, multiply_flow, divide_flow],
        ... )
        >>> flow = Flow.from_steps([parallel_flow_step])
        >>> conversation = flow.start_conversation(inputs={"a": 16, "b": 4})
        >>> status = conversation.execute()

        """
        if len(flows) == 0:
            raise ValueError("ParallelFlowExecutionStep was passed an empty list of sub-flows")

        if any(flow.might_yield for flow in flows):
            raise ValueError("Flows ran in `ParallelFlowExecutionStep` cannot yield")

        super().__init__(
            step_static_configuration=dict(
                flows=flows,
                max_workers=max_workers,
            ),
            input_mapping=input_mapping,
            output_mapping=output_mapping,
            input_descriptors=input_descriptors,
            output_descriptors=output_descriptors,
            name=name,
            __metadata_info__=__metadata_info__,
        )

        self.flows: List["Flow"] = flows
        self.max_workers = max_workers

    def sub_flows(self) -> List["Flow"]:
        return self.flows

    @classmethod
    def _get_step_specific_static_configuration_descriptors(
        cls,
    ) -> Dict[str, type]:
        from wayflowcore.flow import Flow

        return {
            "flows": List[Flow],
            "max_workers": Optional[int],  # type: ignore
        }

    @classmethod
    def _compute_step_specific_input_descriptors_from_static_config(
        cls,
        flows: List["Flow"],
        max_workers: Optional[int],
    ) -> List[Property]:
        input_descriptors_dict = {}
        for flow in flows:
            for input_descriptor in flow.input_descriptors:
                if input_descriptor.name not in input_descriptors_dict:
                    input_descriptors_dict[input_descriptor.name] = input_descriptor
                else:
                    previous_input_descriptor = input_descriptors_dict[input_descriptor.name]
                    # Inputs with the same name must have the same type
                    if not json_schemas_have_same_type(
                        input_descriptor._type_to_json_schema(),  # type: ignore
                        previous_input_descriptor._type_to_json_schema(),  # type: ignore
                    ):
                        raise ValueError(
                            f"Two subflows in ParallelFlowExecutionStep have an input descriptor "
                            f"with the same name but different types:\n"
                            f"1) {input_descriptor}\n2) {previous_input_descriptor}"
                        )
                    logger.info(
                        f"Two sub-flows have the same input descriptor: {input_descriptor}. "
                        f"Make sure they correspond to the same input."
                    )

        return list(input_descriptors_dict.values())

    @classmethod
    def _compute_step_specific_output_descriptors_from_static_config(
        cls,
        flows: List["Flow"],
        max_workers: Optional[int],
    ) -> List[Property]:
        output_descriptors_dict: Dict[str, Property] = {}
        for flow in flows:
            for output_descriptor in flow.output_descriptors:
                if output_descriptor.name not in output_descriptors_dict:
                    output_descriptors_dict[output_descriptor.name] = output_descriptor
                else:
                    raise ValueError(
                        "Output descriptors of subflows inside ParallelFlowExecutionStep cannot have "
                        f"the same name, but found two output descriptors called `{output_descriptor.name}`."
                    )
        return list(output_descriptors_dict.values())

    # override
    @property
    def might_yield(self) -> bool:
        """
        Indicates if the step might yield back to the user.
        It depends on the subflow we are executing
        """
        return False

    async def _invoke_step_async(
        self,
        inputs: Dict[str, Any],
        conversation: "FlowConversation",
    ) -> StepResult:

        sub_conversations = [
            conversation._get_or_create_current_sub_conversation(
                step=self,
                flow=flow,
                inputs={
                    input_name: input_value
                    for input_name, input_value in inputs.items()
                    if input_name in set(p.name for p in flow.input_descriptors)
                    # We extract the inputs needed by this specific flow
                },
                sub_conversation_id=flow.id,
            )
            for flow in self.flows
        ]

        # We run only the conversations that did not reach the end
        sub_conversations_to_run = [
            sub_conversation
            for sub_conversation in sub_conversations
            if sub_conversation.status is None or not isinstance(sub_conversation, FinishedStatus)
        ]
        # We collect the statuses of the conversations that did already finish as we need them for the cleanup
        finished_statuses = [
            sub_conversation.status
            for sub_conversation in sub_conversations
            if sub_conversation.status is not None and isinstance(sub_conversation, FinishedStatus)
        ]

        async def _run_single_flow_target(sub_conv: "FlowConversation") -> ExecutionStatus:
            status = await sub_conv.execute_async()
            return status

        # if no max_workers was given, we use a default number of workers
        max_workers = self.max_workers or _MAX_NUM_WORKERS

        new_statuses = await run_async_function_in_parallel(
            func_async=_run_single_flow_target,
            input_list=sub_conversations_to_run,
            max_workers=max_workers,
        )
        statuses = new_statuses + finished_statuses

        interrupt_status = [
            status for status in statuses if isinstance(status, InterruptedExecutionStatus)
        ]
        if len(interrupt_status) > 0:
            if len(interrupt_status) > 1:
                warnings.warn(
                    f"Multiple subflow executions in ParallelFlowExecutionStep `{self.name}` raised and interrupt, "
                    f"but only one interrupt status at a time can be captured. The first interrupt received is returned."
                )
            return StepResult(
                # We return the status so that it can be propagated
                outputs={"__execution_status__": interrupt_status[0]},
                branch_name=self.BRANCH_SELF,
                step_type=StepExecutionStatus.INTERRUPTED,
            )

        non_finished_status = [
            status for status in statuses if not isinstance(status, FinishedStatus)
        ]
        if len(non_finished_status) > 0:
            raise ValueError(
                f"Illegal response from a subflow: some subflow returned a non-finished status: {non_finished_status}"
            )

        for sub_conv in sub_conversations:
            FlowConversationExecutor().cleanup_sub_conversation(
                sub_conv.state,
                self,
            )

        if not all(isinstance(status, FinishedStatus) for status in statuses):
            raise RuntimeError(
                "Unexpected status: a flow execution inside ParallelFlowExecutionStep was not completed, "
                "but step execution is ending."
            )

        all_outputs: List[Dict[str, Any]] = [status.output_values for status in statuses]  # type: ignore
        output_result: Dict[str, List[Any]] = {
            output_name: output_val
            for single_outputs in all_outputs
            for output_name, output_val in single_outputs.items()
        }

        return StepResult(outputs=output_result)

    def _referenced_tools_dict_inner(
        self, recursive: bool, visited_set: Set[str]
    ) -> Dict[str, "Tool"]:
        all_tools = {}

        if recursive:
            for flow in self.flows:
                all_tools.update(
                    flow._referenced_tools_dict(recursive=True, visited_set=visited_set)
                )

        return all_tools
