# Copyright Â© 2025 Oracle and/or its affiliates.
#
# This software is under the Apache License 2.0
# (LICENSE-APACHE or http://www.apache.org/licenses/LICENSE-2.0) or Universal Permissive License
# (UPL) 1.0 (LICENSE-UPL or https://oss.oracle.com/licenses/upl), at your option.

import json
from textwrap import dedent
from typing import Annotated, Optional, Tuple

import pytest

from wayflowcore._utils._templating_helpers import render_template
from wayflowcore.agent import Agent
from wayflowcore.executors._agentexecutor import _TALK_TO_USER_TOOL_NAME
from wayflowcore.executors._swarmconversation import SwarmConversation
from wayflowcore.executors._swarmexecutor import _HANDOFF_TOOL_NAME, _SEND_MESSAGE_TOOL_NAME
from wayflowcore.executors.executionstatus import (
    FinishedStatus,
    ToolExecutionConfirmationStatus,
    ToolRequestStatus,
    UserMessageRequestStatus,
)
from wayflowcore.flow import Flow
from wayflowcore.messagelist import Message, MessageType
from wayflowcore.property import StringProperty
from wayflowcore.serialization import deserialize, serialize
from wayflowcore.steps import OutputMessageStep
from wayflowcore.swarm import Swarm
from wayflowcore.tools import ClientTool, ToolResult, tool

from .testhelpers.dummy import DummyModel
from .testhelpers.testhelpers import retry_test


@tool
def fooza_tool(
    a: Annotated[int, "first required integer"], b: Annotated[int, "second required integer"]
) -> int:
    """Return the result of the fooza operation between numbers a and b. Do not use for anything else than computing a fooza operation."""
    return a * 2 + b * 3 - 1


@tool
def bwip_tool(
    a: Annotated[int, "first required integer"], b: Annotated[int, "second required integer"]
) -> int:
    """Return the result of the bwip operation between numbers a and b. Do not use for anything else than computing a bwip operation."""
    return a - b + 1


@tool
def zbuk_tool(
    a: Annotated[int, "first required integer"], b: Annotated[int, "second required integer"]
) -> int:
    """Return the result of the zbuk operation between numbers a and b. Do not use for anything else than computing a zbuk operation."""
    return a + b * 2


def _get_fooza_agent(llm):
    return Agent(
        custom_instruction=(
            "You are a fooza operation specialist. The fooza operation is a linear transformation, "
            "designed by Mr. Fooza. Tackle the requests you are specialized to tackle, and let other agents take care of the rest."
        ),
        llm=llm,
        tools=[fooza_tool],
        agent_id="fooza_agent",
        name="fooza_agent",
        description="An specialized AI Assistant that can answer any question/request related to the fooza operation.",
    )


def _get_bwip_agent(llm):
    return Agent(
        custom_instruction=(
            "You are a bwip operation specialist. Answer the user/caller requests about the bwip operation, "
            "and their request only (do not attempt to solve unrelated tasks). The bwip operation is a linear transformation, "
            "designed by Mr. Bwip. Only tackle the requests you are specialized to tackle, and let other agents take care of the rest."
        ),
        llm=llm,
        tools=[bwip_tool],
        agent_id="bwip_agent",
        name="bwip_agent",
        description="An specialized AI Assistant that can answer any question/request related to the bwip operation.",
    )


def _get_zbuk_agent(llm):
    return Agent(
        custom_instruction=(
            "You are a zbuk operation specialist. Answer the user/caller requests about the zbuk operation, "
            "and their request only (do not attempt to solve unrelated tasks). The zbuk operation is a linear transformation, "
            "designed by Mr. Zbuk. Only tackle the requests you are specialized to tackle, and let other agents take care of the rest."
        ),
        llm=llm,
        tools=[zbuk_tool],
        agent_id="zbuk_agent",
        name="zbuk_agent",
        description="An specialized AI Assistant that can answer any question/request related to the zbuk operation.",
    )


@pytest.fixture
def example_math_agents(remote_gemma_llm) -> Tuple[Agent, Agent, Agent]:
    llm = remote_gemma_llm
    return (
        _get_bwip_agent(llm),
        _get_zbuk_agent(llm),
        _get_fooza_agent(llm),
    )


def _get_math_swarm(bwip_agent, zbuk_agent, fooza_agent, handoff: bool):
    all_agents = (bwip_agent, zbuk_agent, fooza_agent)
    return Swarm(
        first_agent=fooza_agent,
        relationships=[  # all-to-all topology
            (ag1, ag2) for ag1 in all_agents for ag2 in all_agents if ag1 is not ag2
        ],
        handoff=handoff,
    )


@pytest.fixture
def example_medical_agents(remotely_hosted_llm) -> Tuple[Agent, Agent, Agent]:
    llm = remotely_hosted_llm
    gp_doctor = Agent(
        llm=llm,
        custom_instruction=(
            "You are a general practitioner doctor."
            "Patients might come to you for common issues like flu etc. For these, just prescribe something directly "
            "For more complicated cases, refer to a specialist."
        ),
        name="GeneralistDoctor",
        description="Generalist practitioner doctor",
    )

    neurologist_doctor = Agent(
        llm=llm,
        custom_instruction=dedent(
            "You are a neurologist. If a patient has cancer, refer them to the oncologist."
        ),
        name="NeurologistDoctor",
        description="Neurologist doctor",
    )

    oncologist_doctor = Agent(
        llm=llm,
        custom_instruction=dedent(
            "You are an oncologist. "
            "You should help patients with cancer, prescribing treatments and giving them precise information"
        ),
        name="OncologistDoctor",
        description="Oncologist doctor",
    )

    return gp_doctor, neurologist_doctor, oncologist_doctor


def test_can_execute_swarm_with_initial_params_passed_in_start_conversation(
    example_medical_agents, remotely_hosted_llm
):
    _, neurologist_doctor, oncologist_doctor = example_medical_agents

    llm = remotely_hosted_llm
    gp_doctor = Agent(
        llm=llm,
        custom_instruction=(
            "You are a general practitioner doctor for a patient names {{ USER }}."
            "This patient might come to you for common issues like flu etc. For these, just prescribe something directly "
            "For more complicated cases, refer to a specialist."
            "Remember to say hi to this patient when first answering."
        ),
        name="GeneralistDoctor",
        description="Generalist practitioner doctor",
    )

    swarm = Swarm(
        first_agent=gp_doctor,
        relationships=[
            (gp_doctor, neurologist_doctor),
            (gp_doctor, oncologist_doctor),
            (neurologist_doctor, oncologist_doctor),
        ],
    )

    conversation = swarm.start_conversation(
        messages=[
            Message(
                content="My skin has been itching for some about a week, can you help me understand what is going on?",
                message_type=MessageType.USER,
            )
        ],
        inputs={"USER": "Iris"},
        conversation_id="12345",
    )

    conversation.execute()

    # The first message must be not the default message as the init messages are passed.
    assert conversation.get_last_message().content != "Hi! How can I help you?"
    assert conversation.conversation_id == "12345"


def test_can_create_swarm(example_medical_agents):
    gp_doctor, neurologist_doctor, oncologist_doctor = example_medical_agents

    Swarm(
        first_agent=gp_doctor,
        relationships=[
            (gp_doctor, neurologist_doctor),
            (gp_doctor, oncologist_doctor),
            (neurologist_doctor, oncologist_doctor),
        ],
    )


def test_swarm_raises_with_empty_relationship_list(example_medical_agents):
    gp_doctor, neurologist_doctor, oncologist_doctor = example_medical_agents

    with pytest.raises(
        ValueError,
        match="Cannot define an `Swarm` with no relationships between the agents. Use an `Agent` instead.",
    ):
        Swarm(
            first_agent=gp_doctor,
            relationships=[],
        )


def test_swarm_raises_when_agent_have_no_name(example_medical_agents):
    gp_doctor, neurologist_doctor, oncologist_doctor = example_medical_agents
    gp_doctor.name = ""

    with pytest.raises(ValueError, match="Agent .* has no name."):
        Swarm(
            first_agent=gp_doctor,
            relationships=[
                (gp_doctor, neurologist_doctor),
                (gp_doctor, oncologist_doctor),
                (neurologist_doctor, oncologist_doctor),
            ],
        )


def test_swarm_raises_warning_when_duplicate_names(example_medical_agents):
    gp_doctor, neurologist_doctor, oncologist_doctor = example_medical_agents
    neurologist_doctor.name = oncologist_doctor.name

    with pytest.raises(ValueError, match="Found agents with duplicated names"):
        Swarm(
            first_agent=gp_doctor,
            relationships=[
                (gp_doctor, neurologist_doctor),
                (gp_doctor, oncologist_doctor),
                (neurologist_doctor, oncologist_doctor),
            ],
        )


def test_swarm_raises_when_duplicated_relationships(example_medical_agents):
    gp_doctor, neurologist_doctor, oncologist_doctor = example_medical_agents

    with pytest.raises(
        ValueError,
        match="Found duplicated relationship involving agents 'GeneralistDoctor' and 'NeurologistDoctor'",
    ):
        Swarm(
            first_agent=gp_doctor,
            relationships=[
                (gp_doctor, neurologist_doctor),
                (gp_doctor, oncologist_doctor),
                (neurologist_doctor, oncologist_doctor),
                (gp_doctor, neurologist_doctor),  # DUPLICATED
            ],
        )


def test_swarm_raises_when_using_flows(example_medical_agents):
    gp_doctor, neurologist_doctor, oncologist_doctor = example_medical_agents

    flow = Flow.from_steps([OutputMessageStep()], name="my_flow", description="my_flow_description")

    with pytest.raises(
        TypeError,
        match="Only Agents are supported in Swarm, got component of type 'Flow'",
    ):
        Swarm(
            first_agent=gp_doctor,
            relationships=[
                (gp_doctor, flow),
                (gp_doctor, oncologist_doctor),
                (neurologist_doctor, oncologist_doctor),
            ],
        )


@retry_test(max_attempts=3)
@pytest.mark.parametrize(
    argnames="handoff",
    argvalues=[False, True],
    ids=["no_handoff", "with_handoff"],
)
def test_swarm_can_complete_task_without_specialist(example_math_agents, handoff: bool):
    """
    The only two configurations used are:
     * [gemma-3-27b-it][no_handoff]
     * [gemma-3-27b-it][with_handoff]

    The other results are for indicative purposes

    # NO HANDOFF
    [Llama-3.1-8B-Instruct][no_handoff]
    Failure rate:          5 out of 30
    Observed on:           2025-05-12
    Average success time:  1.24 seconds per successful attempt
    Average failure time:  1.09 seconds per failed attempt
    Max attempt:           6
    Justification:         (0.19 ** 6) ~= 4.3 / 100'000

    [Llama-4-Maverick][no_handoff]
    Failure rate:          0 out of 30
    Observed on:           2025-05-12
    Average success time:  1.98 seconds per successful attempt
    Average failure time:  No time measurement
    Max attempt:           3
    Justification:         (0.03 ** 3) ~= 3.1 / 100'000

    [gemma-3-27b-it][no_handoff]
    Failure rate:          0 out of 30
    Observed on:           2025-05-12
    Average success time:  1.84 seconds per successful attempt
    Average failure time:  No time measurement
    Max attempt:           3
    Justification:         (0.03 ** 3) ~= 3.1 / 100'000

    [gpt-4.1-nano][no_handoff]
    Failure rate:          0 out of 30
    Observed on:           2025-05-12
    Average success time:  1.49 seconds per successful attempt
    Average failure time:  No time measurement
    Max attempt:           3
    Justification:         (0.03 ** 3) ~= 3.1 / 100'000

    # WITH HANDOFF
    [Llama-3.1-8B-Instruct][with_handoff]
    Failure rate:          3 out of 30
    Observed on:           2025-05-12
    Average success time:  1.50 seconds per successful attempt
    Average failure time:  0.75 seconds per failed attempt
    Max attempt:           5
    Justification:         (0.12 ** 5) ~= 3.1 / 100'000

    [Llama-4-Maverick][with_handoff]
    Failure rate:          0 out of 30
    Observed on:           2025-05-12
    Average success time:  2.28 seconds per successful attempt
    Average failure time:  No time measurement
    Max attempt:           3
    Justification:         (0.03 ** 3) ~= 3.1 / 100'000

    [gemma-3-27b-it][with_handoff]
    Failure rate:          0 out of 30
    Observed on:           2025-05-12
    Average success time:  1.81 seconds per successful attempt
    Average failure time:  No time measurement
    Max attempt:           3
    Justification:         (0.03 ** 3) ~= 3.1 / 100'000

    [gpt-4.1-nano][with_handoff]
    Failure rate:          1 out of 30
    Observed on:           2025-05-12
    Average success time:  1.44 seconds per successful attempt
    Average failure time:  0.51 seconds per failed attempt
    Max attempt:           4
    Justification:         (0.06 ** 4) ~= 1.5 / 100'000
    """
    math_swarm = _get_math_swarm(*example_math_agents, handoff=handoff)
    conv = math_swarm.start_conversation()  # first agent is fooza
    conv.append_user_message("compute the result the fooza operation of 4 and 5")
    conv.execute()

    last_message = conv.get_last_message()
    assert last_message is not None
    assert "22" in last_message.content


@retry_test(max_attempts=3)
@pytest.mark.parametrize(
    argnames="handoff",
    argvalues=[False, True],
    ids=["no_handoff", "with_handoff"],
)
def test_swarm_can_complete_routing_task(example_math_agents, handoff: bool):
    """
    The only two configurations used are:
     * [gemma-3-27b-it][no_handoff]
     * [gemma-3-27b-it][with_handoff]

    The other results are for indicative purposes.

    # NO HANDOFF
    [Llama-3.1-8B-Instruct][no_handoff]
    Failure rate:          12 out of 30
    Observed on:           2025-05-12
    Average success time:  2.72 seconds per successful attempt
    Average failure time:  2.70 seconds per failed attempt
    Max attempt:           11
    Justification:         (0.41 ** 11) ~= 5.0 / 100'000

    [Llama-4-Maverick][no_handoff]
    Failure rate:          0 out of 30
    Observed on:           2025-05-12
    Average success time:  4.08 seconds per successful attempt
    Average failure time:  No time measurement
    Max attempt:           3
    Justification:         (0.03 ** 3) ~= 3.1 / 100'000

    [gemma-3-27b-it][no_handoff]
    Failure rate:          0 out of 30
    Observed on:           2025-05-12
    Average success time:  4.25 seconds per successful attempt
    Average failure time:  No time measurement
    Max attempt:           3
    Justification:         (0.03 ** 3) ~= 3.1 / 100'000

    [gpt-4.1-nano][no_handoff]
    Failure rate:          0 out of 30
    Observed on:           2025-05-12
    Average success time:  3.63 seconds per successful attempt
    Average failure time:  No time measurement
    Max attempt:           3
    Justification:         (0.03 ** 3) ~= 3.1 / 100'000

    # WITH HANDOFF
    [Llama-3.1-8B-Instruct][with_handoff]
    Failure rate:          5 out of 30
    Observed on:           2025-05-12
    Average success time:  3.40 seconds per successful attempt
    Average failure time:  1.86 seconds per failed attempt
    Max attempt:           6
    Justification:         (0.19 ** 6) ~= 4.3 / 100'000

    [Llama-4-Maverick][with_handoff]
    Failure rate:          0 out of 30
    Observed on:           2025-05-12
    Average success time:  4.45 seconds per successful attempt
    Average failure time:  No time measurement
    Max attempt:           3
    Justification:         (0.03 ** 3) ~= 3.1 / 100'000

    [gemma-3-27b-it][with_handoff]
    Failure rate:          0 out of 30
    Observed on:           2025-05-12
    Average success time:  3.88 seconds per successful attempt
    Average failure time:  No time measurement
    Max attempt:           3
    Justification:         (0.03 ** 3) ~= 3.1 / 100'000

    [gpt-4.1-nano][with_handoff]
    Failure rate:          1 out of 30
    Observed on:           2025-05-12
    Average success time:  3.10 seconds per successful attempt
    Average failure time:  4.83 seconds per failed attempt
    Max attempt:           4
    Justification:         (0.06 ** 4) ~= 1.5 / 100'000
    """
    math_swarm = _get_math_swarm(*example_math_agents, handoff=handoff)
    conv = math_swarm.start_conversation()  # first agent is fooza
    conv.append_user_message("compute the result the zbuk operation of 4 and 5")
    conv.execute()

    last_message = conv.get_last_message()
    assert last_message is not None
    assert "14" in last_message.content


@pytest.mark.skip(
    "just to show task ref perf"
)  # Just to show the task with reference performance results
def test_swarm_can_complete_composition_task(math_swarm_no_handoff: Swarm):
    """
    # NO HANDOFF
    [Llama-3.1-8B-Instruct][no_handoff]
    Failure rate:          15 out of 30
    Observed on:           2025-05-12
    Average success time:  6.73 seconds per successful attempt
    Average failure time:  4.30 seconds per failed attempt
    Max attempt:           14
    Justification:         (0.50 ** 14) ~= 6.1 / 100'000

    [Llama-4-Maverick][no_handoff]
    Failure rate:          2 out of 30
    Observed on:           2025-05-12
    Average success time:  8.37 seconds per successful attempt
    Average failure time:  13.19 seconds per failed attempt
    Max attempt:           4
    Justification:         (0.09 ** 4) ~= 7.7 / 100'000

    [gemma-3-27b-it][no_handoff]
    Failure rate:          3 out of 30
    Observed on:           2025-05-12
    Average success time:  8.13 seconds per successful attempt
    Average failure time:  8.12 seconds per failed attempt
    Max attempt:           5
    Justification:         (0.12 ** 5) ~= 3.1 / 100'000

    [gpt-4.1-nano][no_handoff]
    Failure rate:          25 out of 30
    Observed on:           2025-05-12
    Average success time:  6.30 seconds per successful attempt
    Average failure time:  3.94 seconds per failed attempt
    Max attempt:           45
    Justification:         (0.81 ** 45) ~= 8.8 / 100'000

    [gpt-4.1-mini][no_handoff]
    Failure rate:          5 out of 30
    Observed on:           2025-05-12
    Average success time:  11.99 seconds per successful attempt
    Average failure time:  9.07 seconds per failed attempt
    Max attempt:           6
    Justification:         (0.19 ** 6) ~= 4.3 / 100'000

    [gpt4.1][no_handoff]
    Failure rate:          0 out of 10
    Observed on:           2025-05-08
    Average success time:  1.06 seconds per successful attempt
    Average failure time:  No time measurement
    Max attempt:           4
    Justification:         (0.08 ** 4) ~= 4.8 / 100'000

    # WITH HANDOFF
    [Llama-3.1-8B-Instruct][with_handoff]
    Failure rate:          19 out of 30
    Observed on:           2025-05-12
    Average success time:  6.84 seconds per successful attempt
    Average failure time:  5.66 seconds per failed attempt
    Max attempt:           20
    Justification:         (0.62 ** 20) ~= 8.3 / 100'000

    [Llama-4-Maverick][with_handoff]
    Failure rate:          1 out of 30
    Observed on:           2025-05-12
    Average success time:  8.87 seconds per successful attempt
    Average failure time:  11.15 seconds per failed attempt
    Max attempt:           4
    Justification:         (0.06 ** 4) ~= 1.5 / 100'000

    [gemma-3-27b-it][with_handoff]
    Failure rate:          3 out of 30
    Observed on:           2025-05-12
    Average success time:  8.17 seconds per successful attempt
    Average failure time:  8.06 seconds per failed attempt
    Max attempt:           5
    Justification:         (0.12 ** 5) ~= 3.1 / 100'000

    [gpt-4.1-nano][with_handoff]
    Failure rate:          23 out of 30
    Observed on:           2025-05-12
    Average success time:  6.24 seconds per successful attempt
    Average failure time:  4.67 seconds per failed attempt
    Max attempt:           33
    Justification:         (0.75 ** 33) ~= 7.5 / 100'000

    [gpt-4.1-mini][with_handoff]
    Failure rate:          4 out of 30
    Observed on:           2025-05-12
    Average success time:  10.89 seconds per successful attempt
    Average failure time:  7.21 seconds per failed attempt
    Max attempt:           5
    Justification:         (0.16 ** 5) ~= 9.3 / 100'000
    """
    math_swarm = math_swarm_no_handoff
    conv = math_swarm.start_conversation()  # first agent is fooza
    conv.append_user_message("compute the result of the bwip(4, zbuk(5, 6))")
    conv.execute()

    last_message = conv.get_last_message()
    assert last_message is not None
    assert "-12" in last_message.content


_SWARM_TOOL_CALL_TEMPLATE = """
{{thoughts}}

{"name": {{tool_name}}, "parameters": {{tool_params}}}
""".strip()


def _send_message(
    recipient_agent: Agent, message: Optional[str] = None, thoughts: Optional[str] = None
) -> Message:
    return Message(
        render_template(
            _SWARM_TOOL_CALL_TEMPLATE,
            inputs=dict(
                thoughts=thoughts or "THOUGHTS",
                tool_name=_SEND_MESSAGE_TOOL_NAME,
                tool_params=json.dumps(
                    dict(message=message or "MESSAGE", recipient=recipient_agent.name)
                ),
            ),
        ),
        message_type=MessageType.AGENT,
    )


def _handoff_message(recipient_agent: Agent, thoughts: Optional[str] = None) -> Message:
    return Message(
        render_template(
            _SWARM_TOOL_CALL_TEMPLATE,
            inputs=dict(
                thoughts=thoughts or "THOUGHTS",
                tool_name=_HANDOFF_TOOL_NAME,
                tool_params=json.dumps(dict(recipient=recipient_agent.name)),
            ),
        ),
        message_type=MessageType.AGENT,
    )


def test_swarm_warns_agent_on_sending_message_to_caller_instead_of_using_talk_to_user_tool():
    llm = DummyModel()

    agent1 = Agent(llm, name="agent1", description="agent 1")
    agent2 = Agent(llm, name="agent2", description="agent 2")

    swarm = Swarm(first_agent=agent1, relationships=[(agent1, agent2), (agent2, agent1)])
    llm.set_next_output(
        [
            _send_message(
                agent2, message="hey agent 2, can you do ...", thoughts="sending message to agent2"
            ),
            _send_message(
                agent1, message="hey agent 1, can you do ...", thoughts="sending message to agent1"
            ),
        ]
    )

    conv = swarm.start_conversation()
    conv.append_user_message("dummy")
    with pytest.raises(ValueError, match="Did you forget to set the output of the Dummy model"):
        # controlled execution
        conv.execute()

    agent1_agent2_message_list = conv.state.agents_and_threads[agent1.name][
        agent2.name
    ].message_list
    last_message = (
        agent1_agent2_message_list.get_last_message()
    )  # Message warning the `agent2` about what it is doing wrong
    assert (
        f"Circular calling warning: Cannot use {_SEND_MESSAGE_TOOL_NAME} on a caller/user. Please use {_TALK_TO_USER_TOOL_NAME} instead"
        in last_message.content
    )


def test_swarm_raises_on_missing_relationship_when_using_handoff():
    llm = DummyModel()

    agent1 = Agent(llm, name="agent1", description="agent 1")
    agent2 = Agent(llm, name="agent2", description="agent 2")
    agent3 = Agent(llm, name="agent3", description="agent 3")

    swarm = Swarm(
        first_agent=agent1,
        relationships=[
            (agent1, agent2),
            # (agent1, agent3), # required to support handoff
            (agent2, agent3),
            (agent3, agent1),
        ],
    )

    llm.set_next_output(
        [
            _send_message(
                agent2, message="hey agent 2, can you do ...", thoughts="sending message to agent2"
            ),
            _handoff_message(agent3, thoughts="handing off conversation to agent3"),
            _send_message(
                agent1, message="hey agent 1, can you do ...", thoughts="sending message to agent1"
            ),
        ]
    )

    conv = swarm.start_conversation()
    conv.append_user_message("dummy")
    with pytest.raises(
        KeyError,
        match=f"Cannot handoff conversation from .*'{agent1.name}', recipient='{agent2.name}'.*'{agent1.name}', recipient='{agent3.name}'.*'{agent1.name}', '{agent3.name}'",
    ):
        conv.execute()


def test_circular_calling_warning_with_handoff():
    llm = DummyModel()

    agent1 = Agent(llm, name="agent1", description="agent 1")
    agent2 = Agent(llm, name="agent2", description="agent 2")
    agent3 = Agent(llm, name="agent3", description="agent 3")

    swarm = Swarm(
        first_agent=agent1,
        relationships=[
            (agent1, agent2),
            (agent1, agent3),  # required to support handoff
            (agent2, agent3),
            (agent3, agent1),
        ],
    )

    llm.set_next_output(
        [
            _send_message(
                agent2, message="hey agent 2, can you do ...", thoughts="sending message to agent2"
            ),
            _handoff_message(agent3, thoughts="handing off conversation to agent3"),
            _send_message(
                agent1, message="hey agent 1, can you do ...", thoughts="sending message to agent1"
            ),
        ]
    )

    conv = swarm.start_conversation()
    conv.append_user_message("dummy")

    with pytest.raises(ValueError, match="Did you forget to set the output of the Dummy model"):
        # controlled execution
        conv.execute()

    agent1_agent3_message_list = conv.state.agents_and_threads[agent1.name][
        agent3.name
    ].message_list
    last_message = (
        agent1_agent3_message_list.get_last_message()
    )  # Message warning the `agent3` about what it is doing wrong
    assert (
        f"Circular calling warning: Cannot use {_SEND_MESSAGE_TOOL_NAME} on a caller/user. Please use {_TALK_TO_USER_TOOL_NAME} instead"
        in last_message.content
    )


@retry_test(max_attempts=3)
def test_swarm_execution_does_not_raise_errors(remotely_hosted_llm) -> None:
    """
    IMPORTANT: THIS TEST SHOULD ALWAYS HAVE A FAILURE RATE OF 0%
    If any flakiness is detected please contact the core team.

    Failure rate:          0 out of 50
    Observed on:           2025-06-02
    Average success time:  3.58 seconds per successful attempt
    Average failure time:  No time measurement
    Max attempt:           3
    Justification:         (0.02 ** 3) ~= 0.7 / 100'000
    """

    llm = remotely_hosted_llm
    addition_agent = Agent(
        name="addition_agent",
        description="Agent that can do additions",
        llm=llm,
        custom_instruction="You can do additions.",
    )
    multiplication_agent = Agent(
        name="multiplication_agent",
        description="Agent that can do multiplication",
        llm=llm,
        custom_instruction="You can do multiplication.",
    )
    division_agent = Agent(
        name="division_agent",
        description="Agent that can do division",
        llm=llm,
        custom_instruction="You can do division.",
    )

    swarm = Swarm(
        first_agent=addition_agent,
        relationships=[
            (addition_agent, multiplication_agent),
            (addition_agent, division_agent),
            (multiplication_agent, division_agent),
        ],
    )
    conversation = swarm.start_conversation()
    conversation.append_user_message("Please compute 2*2+1")
    conversation.execute()


@tool(requires_confirmation=True, description_mode="only_docstring")
def check_name_in_db_tool(name: str) -> str:
    """Check if a name is present in the database"""
    return "This name is present in the database"


@retry_test(max_attempts=3, wait_between_tries=1)
def test_swarm_can_handle_server_tool_with_confirmation(big_llama):
    """
    Failure rate:          0 out of 50
    Observed on:           2025-09-22
    Average success time:  21.96 seconds per successful attempt
    Average failure time:  No time measurement
    Max attempt:           3
    Justification:         (0.02 ** 3) ~= 0.7 / 100'000
    """
    # Plug the tool into an agent
    llm = big_llama
    main_agent = Agent(
        llm=llm,
        description="general agent which will route queries to your agents",
        custom_instruction="You are a general agent which will always route queries to your agents and collect tool outputs",
    )
    agent = Agent(
        llm=llm,
        tools=[check_name_in_db_tool],
        name="check_name_in_db_agent",
        description="A helpful agent that has access to a tool which check if a given name is present in database.",
        custom_instruction="You should only use one tool at a time. Only use the talk_to_user tool to ask the user questions, not to inform them about your next action. Don't talk to the user unless reporting on the requested tasks.",
    )
    swarm = Swarm(
        first_agent=main_agent,
        relationships=[(main_agent, agent)],
    )

    conv = swarm.start_conversation()
    conv.append_user_message("Is the name Alice present in the database? Ask your agents if needed")
    status = conv.execute()
    assert isinstance(status, ToolExecutionConfirmationStatus)
    assert len(status.tool_requests) == 1
    req = status.tool_requests[0]
    assert req.name == "check_name_in_db_tool"

    # Confirm the tool execution
    status.confirm_tool_execution(tool_request=req)
    status2 = conv.execute()
    # Should result in a user message request or final status
    assert isinstance(status2, UserMessageRequestStatus) or isinstance(status2, FinishedStatus)

    # Now test rejection path
    conv = swarm.start_conversation()
    conv.append_user_message("Is the name Bob present in the database? Ask your agents if needed")
    status = conv.execute()
    assert isinstance(status, ToolExecutionConfirmationStatus)
    # Reject with a reason
    status.reject_tool_execution(
        tool_request=status.tool_requests[0], reason="Invalid request. Do not try again"
    )
    status2 = conv.execute()
    loop_count = 1
    while isinstance(status2, ToolExecutionConfirmationStatus):
        status2.reject_tool_execution(
            tool_request=status2.tool_requests[0],
            reason="Permission Denied, Do not try to use this tool.",
        )
        status2 = conv.execute()
        loop_count += 1
        if loop_count > 3:
            break
    # Should result in a user message request or a graceful finish
    assert isinstance(status2, UserMessageRequestStatus) or isinstance(status2, FinishedStatus)


@retry_test(max_attempts=4)
def test_swarm_can_handle_client_tool_with_confirmation(big_llama):
    """
    Failure rate:          0 out of 50
    Observed on:           2025-10-20
    Average success time:  14.36 seconds per successful attempt
    Average failure time:  No time measurement
    Max attempt:           3
    Justification:         (0.02 ** 3) ~= 0.7 / 100'000
    """
    check_name_in_db_tool = ClientTool(
        name="check_name_in_db_tool",
        description="Check if a name is present in the database",
        input_descriptors=[
            StringProperty("name", description="name to check"),
        ],
        requires_confirmation=True,
    )
    # Plug the tool into an agent
    llm = big_llama
    main_agent = Agent(
        llm=llm,
        description="general agent which will route queries to your agents",
        custom_instruction="You are a general agent which will always route queries to your agents and collect tool outputs",
    )
    agent = Agent(
        llm=llm,
        tools=[check_name_in_db_tool],
        name="check_name_in_db_agent",
        description="A helpful agent that has access to a tool which check if a given name is present in database.",
        custom_instruction="You are an agent which checks if a name is present in the database",
    )
    swarm = Swarm(
        first_agent=main_agent,
        relationships=[(main_agent, agent)],
    )

    conv = swarm.start_conversation()
    conv.append_user_message("Is the name Alice present in the database? Ask your agents if needed")
    status = conv.execute()
    assert isinstance(status, ToolExecutionConfirmationStatus)
    assert len(status.tool_requests) == 1
    status.confirm_tool_execution(tool_request=status.tool_requests[0])
    status = conv.execute()
    assert isinstance(status, ToolRequestStatus)
    assert len(status.tool_requests) == 1
    req = status.tool_requests[0]
    assert req.name == "check_name_in_db_tool"


@retry_test(max_attempts=4)
def test_swarm_can_handle_client_tool(big_llama):
    """
    Failure rate:          2 out of 50
    Observed on:           2025-10-09
    Average success time:  10.91 seconds per successful attempt
    Average failure time:  8.84 seconds per failed attempt
    Max attempt:           4
    Justification:         (0.06 ** 4) ~= 1.1 / 100'000
    """
    check_name_in_db_tool = ClientTool(
        name="check_name_in_db_tool",
        description="Check if a name is present in the database",
        input_descriptors=[
            StringProperty("name", description="name to check"),
        ],
    )
    # Plug the tool into an agent
    llm = big_llama
    main_agent = Agent(
        llm=llm,
        description="general agent which will route queries to your agents",
        custom_instruction="You are a general agent which will always route queries to your agents and collect tool outputs",
    )
    agent = Agent(
        llm=llm,
        tools=[check_name_in_db_tool],
        name="check_name_in_db_agent",
        description="A helpful agent that has access to a tool which check if a given name is present in database.",
        custom_instruction="You are an agent which checks if a name is present in the database",
    )
    swarm = Swarm(
        first_agent=main_agent,
        relationships=[(main_agent, agent)],
    )

    conv = swarm.start_conversation()
    conv.append_user_message("Is the name Alice present in the database? Ask your agents if needed")
    status = conv.execute()
    assert isinstance(status, ToolRequestStatus)
    assert len(status.tool_requests) == 1
    req = status.tool_requests[0]
    assert req.name == "check_name_in_db_tool"

    # Check conversation can be continued after being serialized and deserialized
    serialized_conv = serialize(conv)
    conv = deserialize(SwarmConversation, serialized_conv)

    # Confirm the tool execution
    tool_result = "The name Alice is present in the database"
    conv.append_tool_result(ToolResult(content=tool_result, tool_request_id=req.tool_request_id))
    status_2 = conv.execute()
    assert isinstance(status_2, UserMessageRequestStatus) or isinstance(status_2, FinishedStatus)
