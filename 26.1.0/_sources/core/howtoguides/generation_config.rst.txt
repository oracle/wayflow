.. _top-howto-generationconfig:

===========================================================
How to Specify the Generation Configuration when Using LLMs
===========================================================


.. |python-icon| image:: ../../_static/icons/python-icon.svg
   :width: 40px
   :height: 40px

.. grid:: 2

    .. grid-item-card:: |python-icon| Download Python Script
        :link: ../end_to_end_code_examples/example_generationconfig.py
        :link-alt: Generation Configuration how-to script

        Python script/notebook for this guide.

Generation parameters, such as temperature, top-p, and the maximum number of output tokens, are important for achieving the desired performance with Large Language Models (LLMs).
In WayFlow, these parameters can be configured with the :ref:`LlmGenerationConfig <llmgenerationconfig>` class.

This guide will show you how to:

- Configure the generation parameters for an agent.
- Configure the generation parameters for a flow.
- Apply the generation configuration from a dictionary.
- Save a custom generation configuration.

.. note::
    For a deeper understanding of the impact of each generation parameter, refer to the resources at the bottom of this page.


Basic implementation
====================

Configure the generation parameters for an agent
------------------------------------------------

Customizing the generation configuration for an agent requires the use of the following ``wayflowcore`` components.

.. literalinclude:: ../code_examples/example_generationconfig.py
    :language: python
    :start-after: .. start-##_Imports
    :end-before: .. end-##_Imports

The generation configuration can be specified when initializing the LLM using the :ref:`LlmGenerationConfig <llmgenerationconfig>` class.
This ensures that all the outputs generated by the agent will have the same generation configuration.

The generation configuration dictionary can have the following arguments:

- ``max_new_tokens``: controls the maximum numbers of tokens to generate, ignoring the number of tokens in the prompt;
- ``temperature``: controls the randomness of the output;
- ``top_p``: controls the randomness of the output;
- ``stop``: defines a list of stop words to indicate the LLM to stop generating;
- ``frequency_penalty``: controls the frequency of tokens generated.

Additionally, the :ref:`LlmGenerationConfig <llmgenerationconfig>` offers the possibility to set a dictionary
of arbitrary parameters, called ``extra_args``, that will be sent as part of the llm generation call.
This allows specifying provider-specific parameters that might not be common to all.

.. note::
    The extra parameters should never include sensitive information.

.. literalinclude:: ../code_examples/example_generationconfig.py
    :language: python
    :start-after: .. start-##_Define_the_llm_generation_configuration
    :end-before: .. end-##_Define_the_llm_generation_configuration

WayFlow supports several LLM API providers.
You can pass the ``generation_config`` for each of them.
Select an LLM from the options below:

.. include:: ../_components/llm_config_tabs.rst

.. important::
    API keys should not be stored anywhere in the code. Use environment variables and/or tools such as `python-dotenv <https://pypi.org/project/python-dotenv/>`_

Now, you can build an agent using the LLM as follows:

.. literalinclude:: ../code_examples/example_generationconfig.py
    :language: python
    :start-after: .. start-##_Build_the_agent_and_run_it
    :end-before: .. end-##_Build_the_agent_and_run_it

Configure the generation parameters for a flow
----------------------------------------------

Customizing the generation configuration for a flow requires the use of the following ``wayflowcore`` components.

.. literalinclude:: ../code_examples/example_generationconfig.py
    :language: python
    :start-after: .. start-##_Import_what_is_needed_to_build_a_flow
    :end-before: .. end-##_Import_what_is_needed_to_build_a_flow

Refer to the previous section to learn how to configure the generation parameters when initializing an LLM
using the :ref:`LlmGenerationConfig <llmgenerationconfig>` class.

You can then create a one-step flow using the :ref:`PromptExecutionStep <promptexecutionstep>` step.

.. literalinclude:: ../code_examples/example_generationconfig.py
    :language: python
    :start-after: .. start-##_Build_the_flow_using_custom_generation_parameters
    :end-before: .. end-##_Build_the_flow_using_custom_generation_parameters

.. important::
    The ``generation_config`` parameter passed to the ``PromptExecutionStep`` overrides the LLM's original generation configuration.

Advanced usage
==============

The :ref:`LlmGenerationConfig <llmgenerationconfig>` class is a serializable object. It can be instantiated from a dictionary or saved to one, as you will see below.

Apply the generation configuration from a dictionary
----------------------------------------------------

If you have a generation configuration in a dictionary (for example, from a JSON or YAML file),
you can instantiate the :ref:`LlmGenerationConfig <llmgenerationconfig>` class as follows:

.. literalinclude:: ../code_examples/example_generationconfig.py
    :language: python
    :start-after: .. start-##_Build_the_generation_configuration_from_dictionary
    :end-before: .. end-##_Build_the_generation_configuration_from_dictionary


Save a custom generation configuration
--------------------------------------

If you would like to share your specific generation configuration, you can create a :ref:`LlmGenerationConfig <llmgenerationconfig>`
class instance and store it to a dictionary.

.. literalinclude:: ../code_examples/example_generationconfig.py
    :language: python
    :start-after: .. start-##_Export_a_generation_configuration_to_dictionary
    :end-before: .. end-##_Export_a_generation_configuration_to_dictionary


Agent Spec Exporting/Loading
============================

You can export the assistant configuration to its Agent Spec configuration using the ``AgentSpecExporter``.
The following example exports the serialization of the flow defined above.

.. literalinclude:: ../code_examples/example_generationconfig.py
    :language: python
    :start-after: .. start-##_Export_config_to_Agent_Spec
    :end-before: .. end-##_Export_config_to_Agent_Spec

Here is what the **Agent Spec representation will look like â†“**

.. collapse:: Click here to see the assistant configuration.

   .. tabs::

      .. tab:: JSON

         .. literalinclude:: ../config_examples/example_generationconfig.json
            :language: json

      .. tab:: YAML

         .. literalinclude:: ../config_examples/example_generationconfig.yaml
            :language: yaml

You can then load the configuration back to an assistant using the ``AgentSpecLoader``.

.. literalinclude:: ../code_examples/example_generationconfig.py
    :language: python
    :start-after: .. start-##_Load_Agent_Spec_config
    :end-before: .. end-##_Load_Agent_Spec_config


Next steps
==========

Having learned how to specify the generation configuration, you may now proceed to:

- :doc:`API Reference on LLM Models <../api/llmmodels>`
- :doc:`Build a Simple Conversational Assistant with Agents <../tutorials/basic_agent>`
- :doc:`How to Create Conditional Transitions in Flows <conditional_flows>`
- :doc:`How to Build Assistants with Tools <howto_build_assistants_with_tools>`

Some additional resources we recommend:

- `HuggingFace - Generation with LLMs <https://huggingface.co/docs/transformers/llm_tutorial>`_
- `HuggingFace - Text generation strategies <https://huggingface.co/docs/transformers/generation_strategies>`_


Full code
=========

Click on the card at the :ref:`top of this page <top-howto-generationconfig>` to download the full code for this guide or copy the code below.

.. literalinclude:: ../end_to_end_code_examples/example_generationconfig.py
    :language: python
    :linenos:
