.. _top-howtoprompttemplates:

========================================
How to Use Advanced Prompting Techniques
========================================


.. |python-icon| image:: ../../_static/icons/python-icon.svg
   :width: 40px
   :height: 40px

.. grid:: 2

    .. grid-item-card:: |python-icon| Download Python Script
        :link: ../end_to_end_code_examples/howto_prompttemplate.py
        :link-alt: Prompt templates how-to script

        Python script/notebook for this guide.



.. admonition:: Prerequisites

    This guide assumes familiarity with:

    - :doc:`LLMs <../api/llmmodels>`
    - :doc:`Agent <../api/agent>`

:doc:`PromptTemplate <../api/prompttemplate>` is a powerful way to configure how to prompt various LLMs.
Although WayFlow has very good defaults for most tasks and LLM providers, some specific use cases or providers require adapting prompt formatting to improve efficiency (e.g., token cost) or performance.
To address this, WayFlow introduces the concept of ``PromptTemplate``, which enables you to define prompts in a very generic way, applicable across different scenarios.

This guide will show you how to:

- Recap how to use variables in prompts
- Learn how to configure a custom ``PromptTemplate``
- Learn how to configure a prompt template for an ``Agent``
- Learn best practices for writing prompts

Basics of prompting
===================

WayFlow leverages LLMs to perform chat completion, meaning that given a list of messages (which can be ``system`` (for instructions to the LLM),
``agent`` (a message generated by the LLM), and ``user`` (a message from the user)), it returns a new message.

To perform agentic tasks, LLMs are equipped with ``tools``, which are defined functions with names and arguments that the LLM may decide to call.
Prompting the LLM to generate such calls can be done using ``native_tool_calling``, where the LLM provider handles how tools are presented to the model and how tool calls are parsed,
or through custom tool calling, where the presentation of tools to the model is controlled externally and the raw text output from the model must be parsed manually.

In many use cases, generating raw text is not enough because it is difficult to use in further pipeline steps.
LLMs can produce specific structured output, often through ``structured_generation``.
Most LLM providers support this feature natively (the provider takes care of formatting the expected response and parsing the LLM output),
but you can also do it in a custom way, using a specific prompt combined with custom parsing.

The ``PromptTemplate`` abstraction is designed so that configuring all these parameters is as simple as possible, while guaranteeing maximum customization.
Using ``PromptTemplate`` usually involves 3 steps:

- Create the basic template. This is done using either the constructor (``PromptTemplate()``) or the helper function from a string (``PromptTemplate.from_string()``).
- Equip the template with ``partial values``, ``tools``, or ``response_format``. This can be done dynamically, and is useful when these elements are unknown at template creation but can be reused across multiple generations.
- Render the template: use ``template.format()`` to fill all variables and create a ``Prompt`` object, which can then be passed to an LLM (e.g., ``llm.generate(prompt)``). This object contains all the information needed for the LLM generation.

Configure the LLM
=================

WayFlow supports several LLM API providers.
Select an LLM from the options below:

.. include:: ../_components/llm_config_tabs.rst


Basic text prompt with Regex parsing
====================================

One possible use case is prompting an LLM and extracting some information from the raw text output.
This is especially relevant in a prompt template, since the parsing method highly depends on how the LLM is prompted in the first place.

The example below shows how to write a simple ``Chain-of-Thoughts`` prompt.
The ``RegexOutputParser`` is used to configure how output is extracted from the raw text.

.. literalinclude:: ../code_examples/howto_prompttemplate.py
   :language: python
   :linenos:
   :start-after: .. start-##_Basic_text_prompt_with_Regex_parsing
   :end-before: .. end-##_Basic_text_prompt_with_Regex_parsing

API reference: :ref:`PromptTemplate <PromptTemplate>`,  :ref:`RegexOutputParser <RegexOutputParser>`, :ref:`RegexPattern <RegexPattern>`, :ref:`JsonOutputParser <JsonOutputParser>`.


Prompt with chat history
========================

In many cases—especially when working with agents—it is necessary to format a list of messages within a prompt.
There are two ways to achieve this.
Assume a list of messages is available:

.. literalinclude:: ../code_examples/howto_prompttemplate.py
   :language: python
   :linenos:
   :start-after: .. start-##_Prompt_with_chat_history
   :end-before: .. end-##_Prompt_with_chat_history

The first way is to integrate the history of messages directly into the prompt's list of messages.
This helps the model by presenting the entire conversation as a single context.
For example, to format the conversation as messages within a template, you can use the ``CHAT_HISTORY_PLACEHOLDER``:

.. literalinclude:: ../code_examples/howto_prompttemplate.py
   :language: python
   :linenos:
   :start-after: .. start-###_As_inlined_messages
   :end-before: .. end-###_As_inlined_messages

You can also format the chat history directly in a message, by using the ``CHAT_HISTORY_PLACEHOLDER_NAME`` placeholder (whose underlying value is the string ``"__CHAT_HISTORY__"`` as shown in the code snippet below):

.. literalinclude:: ../code_examples/howto_prompttemplate.py
   :language: python
   :linenos:
   :start-after: .. start-###_In_the_system_prompt
   :end-before: .. end-###_In_the_system_prompt

If you need to filter some part of the chat history before rendering it in the template, you may use some pre-rendering message transforms:

.. literalinclude:: ../code_examples/howto_prompttemplate.py
   :language: python
   :linenos:
   :start-after: .. start-###_With_message_transform
   :end-before: .. end-###_With_message_transform

API reference: :ref:`MessageTransform <MessageTransform>`.

This will ensure only the last chat history message is formatted in the prompt.

Configure how to use tools in templates
=======================================

Assume a tool is available and should be used for LLM generation.

.. literalinclude:: ../code_examples/howto_prompttemplate.py
   :language: python
   :linenos:
   :start-after: .. start-##_Configure_how_to_use_tools_in_templates
   :end-before: .. end-##_Configure_how_to_use_tools_in_templates

If the LLM provider supports  ``native_tool_calling``, integrating it into templates is straightforward:

.. literalinclude:: ../code_examples/howto_prompttemplate.py
   :language: python
   :linenos:
   :start-after: .. start-###_With_native_tool_calling
   :end-before: .. end-###_With_native_tool_calling

In this case, the template uses native tool calling, so the prompt includes separate ``tools`` that are passed to the LLM endpoint.
The provider directly parses the output and returns a ``ToolRequest``.

Sometimes, the provider's native tool calling might not be available or may not work as needed (for example, when performing Chain-of-Thought reasoning with Llama native models, which do not support this feature).
In such cases, tool calling can be configured directly within the prompt template.

.. literalinclude:: ../code_examples/howto_prompttemplate.py
   :language: python
   :linenos:
   :start-after: .. start-###_With_custom_tool_calling
   :end-before: .. end-###_With_custom_tool_calling

For custom tool calling, the important parameters to configure are:

- ``native_tool_calling`` indicates that the tools will be formatted in the prompt. This requires the ``__TOOLS__`` placeholder in one of the messages.
- ``post_rendering_transforms``: transformations applied to messages just before passing them to the LLM. These allow transformations specific to a tool-calling technique or an LLM provider's requirements. The following post-transforms are used here:

   - ``_ReactMergeToolRequestAndCallsTransform`` formats tool calls and tool requests into standard user or agent messages.
   - ``CoalesceSystemMessagesTransform`` allows merging consecutive system messages into a single system message, as required by some LLM providers.
   - ``RemoveEmptyNonUserMessageTransform`` removes empty agent or system messages, since some LLMs do not support them.
- ``output_parser``: specifies how the raw LLM text output is parsed into a tool request. Several examples can be found in the  ``wayflowcore.templates`` subpackage.
- ``generation_config``: for this ReAct-style template, specific generation parameters are added to help reduce hallucinations.


Configure how to use structured generation in templates
=======================================================

Structured generation can be handled using either ``native_tool_calling`` (if the LLM provider supports it) or custom methods.
Assume the goal is to generate the following output:

.. literalinclude:: ../code_examples/howto_prompttemplate.py
   :language: python
   :linenos:
   :start-after: .. start-##_Configure_how_to_use_structured_generation_in_templates
   :end-before: .. end-##_Configure_how_to_use_structured_generation_in_templates

API reference: :ref:`ObjectProperty <ObjectProperty>`, :ref:`StringProperty <StringProperty>`.

With native structured generation
---------------------------------

Some providers allow passing the desired output format separately and ensure the generated output follows the given format.
If the LLM provider supports it, then using a template is straightforward:

.. literalinclude:: ../code_examples/howto_prompttemplate.py
   :language: python
   :linenos:
   :start-after: .. start-###_With_native_structured_generation
   :end-before: .. end-###_With_native_structured_generation

With custom structured generation
---------------------------------

If the LLM provider does not support native structured generation, it is possible to prompt the LLM to produce output in the desired format manually:

.. literalinclude:: ../code_examples/howto_prompttemplate.py
   :language: python
   :linenos:
   :start-after: .. start-###_With_custom_structured_generation
   :end-before: .. end-###_With_custom_structured_generation

The main parameters to configure are:

- ``native_structured_generation=False``: prevents the ``response_format`` from being passed separately. If a ``__RESPONSE_FORMAT__`` placeholder is used, the ``response_format`` will be inserted there.
- ``output_parser``: to correctly parse the raw LLM output into the expected object. In this example, a simple output parser is used that can repair malformed JSON output.

You can also use custom output parser to ensure the expected format.
In such cases, it is beneficial to include the ``response_format`` in the template to clearly indicate the structured output format being generated.
For example, parsing the content of the chain-of-thoughts can be done as follows:

.. literalinclude:: ../code_examples/howto_prompttemplate.py
   :language: python
   :linenos:
   :start-after: .. start-###_With_additional_output_parser
   :end-before: .. end-###_With_additional_output_parser

.. tip::
   Wayflow also provides a helper function to set up a simple JSON structured generation prompt and
   the associated output parser from an existing prompt template leveraging native structured generation.
   Check out the :ref:`helper method's API Reference <prompttemplatehelpers>`.


Agent Spec Exporting/Loading
============================

You can export the assistant configuration to its Agent Spec configuration using the ``AgentSpecExporter``.

.. literalinclude:: ../code_examples/howto_prompttemplate.py
   :language: python
   :start-after: .. start-##_Export_config_to_Agent_Spec
   :end-before: .. end-##_Export_config_to_Agent_Spec


Here is what the **Agent Spec representation will look like ↓**

.. collapse:: Click here to see the assistant configuration.

   .. tabs::

      .. tab:: JSON

         .. literalinclude:: ../config_examples/howto_prompttemplate.json
            :language: json

      .. tab:: YAML

         .. literalinclude:: ../config_examples/howto_prompttemplate.yaml
            :language: yaml

You can then load the configuration back to an assistant using the ``AgentSpecLoader``.

.. literalinclude:: ../code_examples/howto_prompttemplate.py
   :language: python
   :start-after: .. start-##_Load_Agent_Spec_config
   :end-before: .. end-##_Load_Agent_Spec_config


.. note::

    This guide uses the following extension/plugin Agent Spec components:

    - ``PluginPromptTemplate``
    - ``PluginRegexOutputParser``
    - ``ExtendedAgent``

    See the list of available Agent Spec extension/plugin components in the :doc:`API Reference <../api/agentspec>`


.. note::
   By default, all WayFlow serialization/deserialization plugins are used when loading or exporting
   from/to Agent Spec. Passing a list of plugins overrides the default ones.


Next steps
==========

Having learned how to use the ``PromptTemplate``, you may now proceed to using it in :ref:`PromptExecutionStep <promptexecutionstep>` or :ref:`Agents <agent>`.


Full code
=========

Click on the card at the :ref:`top of this page <top-howtoprompttemplates>` to download the full code for this guide or copy the code below.

.. literalinclude:: ../end_to_end_code_examples/howto_prompttemplate.py
   :language: python
   :linenos:
